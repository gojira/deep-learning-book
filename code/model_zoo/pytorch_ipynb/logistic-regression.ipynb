{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Accompanying code examples of the book \"Introduction to Artificial Neural Networks and Deep Learning: A Practical Guide with Applications in Python\" by [Sebastian Raschka](https://sebastianraschka.com). All code examples are released under the [MIT license](https://github.com/rasbt/deep-learning-book/blob/master/LICENSE). If you find this content useful, please consider supporting the work by buying a [copy of the book](https://leanpub.com/ann-and-deeplearning).*\n",
    "  \n",
    "Other code examples and content are available on [GitHub](https://github.com/rasbt/deep-learning-book). The PDF and ebook versions of the book are available through [Leanpub](https://leanpub.com/ann-and-deeplearning)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sebastian Raschka \n",
      "\n",
      "CPython 3.6.3\n",
      "IPython 6.2.1\n",
      "\n",
      "torch 0.3.0.post4\n"
     ]
    }
   ],
   "source": [
    "%load_ext watermark\n",
    "%watermark -a 'Sebastian Raschka' -v -p torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Zoo -- Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementation of *classic* logistic regression for binary class labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing a toy dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1158c5390>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from io import BytesIO\n",
    "\n",
    "##########################\n",
    "### DATASET\n",
    "##########################\n",
    "\n",
    "ds = np.lib.DataSource()\n",
    "fp = ds.open('https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data')\n",
    "\n",
    "x = np.genfromtxt(BytesIO(fp.read().encode()), delimiter=',', usecols=range(2), max_rows=100)\n",
    "y = np.zeros(100)\n",
    "y[50:] = 1\n",
    "\n",
    "np.random.seed(1)\n",
    "idx = np.arange(y.shape[0])\n",
    "np.random.shuffle(idx)\n",
    "X_test, y_test = x[idx[:25]], y[idx[:25]]\n",
    "X_train, y_train = x[idx[25:]], y[idx[25:]]\n",
    "mu, std = np.mean(X_train, axis=0), np.std(X_train, axis=0)\n",
    "X_train, X_test = (X_train - mu) / std, (X_test - mu) / std\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(7, 2.5))\n",
    "ax[0].scatter(X_train[y_train == 1, 0], X_train[y_train == 1, 1])\n",
    "ax[0].scatter(X_train[y_train == 0, 0], X_train[y_train == 0, 1])\n",
    "ax[1].scatter(X_test[y_test == 1, 0], X_test[y_test == 1, 1])\n",
    "ax[1].scatter(X_test[y_test == 0, 0], X_test[y_test == 0, 1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Low-level implementation with manual gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "def custom_where(cond, x_1, x_2):\n",
    "    return (cond * x_1) + ((1-cond) * x_2)\n",
    "\n",
    "\n",
    "class LogisticRegression1():\n",
    "    def __init__(self, num_features):\n",
    "        self.num_features = num_features\n",
    "        self.weights = torch.zeros(num_features, 1)\n",
    "        self.bias = torch.zeros(1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        linear = torch.add(torch.mm(x, self.weights), self.bias)\n",
    "        probas = self._sigmoid(linear)\n",
    "        return probas\n",
    "        \n",
    "    def backward(self, probas, y):  \n",
    "        errors = y - probas.view(-1)\n",
    "        return errors\n",
    "            \n",
    "    def predict_labels(self, x):\n",
    "        probas = self.forward(x)\n",
    "        labels = custom_where(probas >= .5, 1, 0)\n",
    "        return labels    \n",
    "            \n",
    "    def evaluate(self, x, y):\n",
    "        labels = self.predict_labels(x).float()\n",
    "        accuracy = torch.sum(labels.view(-1) == y) / y.size()[0]\n",
    "        return accuracy\n",
    "    \n",
    "    def _sigmoid(self, z):\n",
    "        return 1. / (1. + torch.exp(-z))\n",
    "    \n",
    "    def _logit_cost(self, y, proba):\n",
    "        tmp1 = torch.mm(-y.view(1, -1), torch.log(proba))\n",
    "        tmp2 = torch.mm((1 - y).view(1, -1), torch.log(1 - proba))\n",
    "        return tmp1 - tmp2\n",
    "    \n",
    "    def train(self, x, y, num_epochs, learning_rate=0.01):\n",
    "        for e in range(num_epochs):\n",
    "            \n",
    "            #### Compute outputs ####\n",
    "            probas = self.forward(x)\n",
    "            \n",
    "            #### Compute gradients ####\n",
    "            errors = self.backward(probas, y)\n",
    "            neg_grad = torch.mm(x.transpose(0, 1), errors.view(-1, 1))\n",
    "            \n",
    "            #### Update weights ####\n",
    "            self.weights += learning_rate * neg_grad\n",
    "            self.bias += learning_rate * torch.sum(errors)\n",
    "            \n",
    "            #### Logging ####\n",
    "            print('Epoch: %03d' % (e+1), end=\"\")\n",
    "            print(' | Train ACC: %.3f' % self.evaluate(x, y), end=\"\")\n",
    "            print(' | Cost: %.3f' % self._logit_cost(y, self.forward(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001 | Train ACC: 0.987 | Cost: 5.581\n",
      "Epoch: 002 | Train ACC: 0.987 | Cost: 4.882\n",
      "Epoch: 003 | Train ACC: 1.000 | Cost: 4.381\n",
      "Epoch: 004 | Train ACC: 1.000 | Cost: 3.998\n",
      "Epoch: 005 | Train ACC: 1.000 | Cost: 3.693\n",
      "Epoch: 006 | Train ACC: 1.000 | Cost: 3.443\n",
      "Epoch: 007 | Train ACC: 1.000 | Cost: 3.232\n",
      "Epoch: 008 | Train ACC: 1.000 | Cost: 3.052\n",
      "Epoch: 009 | Train ACC: 1.000 | Cost: 2.896\n",
      "Epoch: 010 | Train ACC: 1.000 | Cost: 2.758\n",
      "\n",
      "Model parameters:\n",
      "  Weights: \n",
      " 4.2267\n",
      "-2.9613\n",
      "[torch.FloatTensor of size 2x1]\n",
      "\n",
      "  Bias: \n",
      "1.00000e-02 *\n",
      "  9.9419\n",
      "[torch.FloatTensor of size 1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "logr = LogisticRegression1(num_features=2)\n",
    "X_train_tensor, y_train_tensor = torch.Tensor(X_train), torch.Tensor(y_train)\n",
    "logr.train(X_train_tensor, y_train_tensor, num_epochs=10, learning_rate=0.1)\n",
    "\n",
    "print('\\nModel parameters:')\n",
    "print('  Weights: %s' % logr.weights)\n",
    "print('  Bias: %s' % logr.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set accuracy: 100.000%\n"
     ]
    }
   ],
   "source": [
    "X_test_tensor, y_test_tensor = torch.Tensor(X_test), torch.Tensor(y_test)\n",
    "\n",
    "test_acc = logr.evaluate(X_test_tensor, y_test_tensor)\n",
    "print('Test set accuracy: %.3f%%' % (test_acc*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Low-level implementation using autograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtype = torch.FloatTensor\n",
    "# dtype = torch.cuda.FloatTensor # Uncomment this to run on GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "def custom_where(cond, x_1, x_2):\n",
    "    return (cond * x_1) + ((1-cond) * x_2)\n",
    "\n",
    "\n",
    "class LogisticRegression2():\n",
    "    def __init__(self, num_features):\n",
    "        self.num_features = num_features\n",
    "        self.weights = Variable(torch.zeros(num_features, 1).type(dtype),\n",
    "                                requires_grad=True)\n",
    "        self.bias = Variable(torch.zeros(1).type(dtype),\n",
    "                             requires_grad=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        linear = torch.add(torch.mm(x, self.weights), self.bias)\n",
    "        probas = self._sigmoid(linear)\n",
    "        return probas\n",
    "                    \n",
    "    def predict_labels(self, x):\n",
    "        probas = self.forward(x)\n",
    "        labels = custom_where((probas >= .5).float(), 1, 0)\n",
    "        return labels    \n",
    "            \n",
    "    def evaluate(self, x, y):\n",
    "        labels = self.predict_labels(x)\n",
    "        accuracy = (torch.sum(labels.view(-1) == y.view(-1))).float() / y.size()[0]\n",
    "        return accuracy\n",
    "    \n",
    "    def _sigmoid(self, z):\n",
    "        return 1. / (1. + torch.exp(-z))\n",
    "    \n",
    "    def _logit_cost(self, y, proba):\n",
    "        tmp1 = torch.mm(-y.view(1, -1), torch.log(proba))\n",
    "        tmp2 = torch.mm((1 - y).view(1, -1), torch.log(1 - proba))\n",
    "        return tmp1 - tmp2\n",
    "    \n",
    "    def train(self, x, y, num_epochs, learning_rate=0.01):\n",
    "        \n",
    "        x_var = Variable(x.type(dtype), requires_grad=False)\n",
    "        y_var = Variable(y.type(dtype), requires_grad=False)\n",
    "        \n",
    "        for e in range(num_epochs):\n",
    "            \n",
    "            #### Compute outputs ####\n",
    "            proba = self.forward(x_var)\n",
    "            cost = self._logit_cost(y_var, proba)\n",
    "            \n",
    "            #### Compute gradients ####\n",
    "            cost.backward()\n",
    "            \n",
    "            #### Update weights ####\n",
    "            self.weights.data -= learning_rate * self.weights.grad.data\n",
    "            self.bias.data -= learning_rate * self.bias.grad.data\n",
    "      \n",
    "            #### Reset gradients to zero for next iteration ####\n",
    "            self.weights.grad.data.zero_()\n",
    "            self.bias.grad.data.zero_()\n",
    "    \n",
    "            #### Logging ####\n",
    "            print('Epoch: %03d' % (e+1), end=\"\")\n",
    "            print(' | Train ACC: %.3f' % self.evaluate(x_var, y_var), end=\"\")\n",
    "            print(' | Cost: %.3f' % self._logit_cost(y_var, self.forward(x_var)))\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001 | Train ACC: 0.987 | Cost: 5.581\n",
      "Epoch: 002 | Train ACC: 0.987 | Cost: 4.882\n",
      "Epoch: 003 | Train ACC: 1.000 | Cost: 4.381\n",
      "Epoch: 004 | Train ACC: 1.000 | Cost: 3.998\n",
      "Epoch: 005 | Train ACC: 1.000 | Cost: 3.693\n",
      "Epoch: 006 | Train ACC: 1.000 | Cost: 3.443\n",
      "Epoch: 007 | Train ACC: 1.000 | Cost: 3.232\n",
      "Epoch: 008 | Train ACC: 1.000 | Cost: 3.052\n",
      "Epoch: 009 | Train ACC: 1.000 | Cost: 2.896\n",
      "Epoch: 010 | Train ACC: 1.000 | Cost: 2.758\n",
      "\n",
      "Model parameters:\n",
      "  Weights: Variable containing:\n",
      " 4.2267\n",
      "-2.9613\n",
      "[torch.FloatTensor of size 2x1]\n",
      "\n",
      "  Bias: Variable containing:\n",
      "1.00000e-02 *\n",
      "  9.9419\n",
      "[torch.FloatTensor of size 1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "logr = LogisticRegression2(num_features=2)\n",
    "X_train_tensor, y_train_tensor = torch.Tensor(X_train), torch.Tensor(y_train)\n",
    "logr.train(X_train_tensor, y_train_tensor, num_epochs=10, learning_rate=0.1)\n",
    "\n",
    "print('\\nModel parameters:')\n",
    "print('  Weights: %s' % logr.weights)\n",
    "print('  Bias: %s' % logr.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set accuracy: 100.00%\n"
     ]
    }
   ],
   "source": [
    "X_test_var = Variable(torch.Tensor(X_test).type(dtype), requires_grad=False)\n",
    "y_test_var = Variable(torch.Tensor(y_test).type(dtype), requires_grad=False)\n",
    "\n",
    "test_acc = logr.evaluate(X_test_var, y_test_var)\n",
    "print('Test set accuracy: %.2f%%' % (test_acc*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## High-level implementation using the nn.Module API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression3(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, num_features):\n",
    "        super(LogisticRegression3, self).__init__()\n",
    "        self.linear = torch.nn.Linear(num_features, 1)\n",
    "        # initialize weights to zeros here,\n",
    "        # since we used zero weights in the\n",
    "        # manual approach\n",
    "        self.linear.weight.data.zero_()\n",
    "        self.linear.bias.data.zero_()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        logits = self.linear(x)\n",
    "        probas = F.sigmoid(logits)\n",
    "        return probas\n",
    "\n",
    "model = LogisticRegression3(num_features=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Define cost function and set up optimizer #####\n",
    "cost_fn = torch.nn.BCELoss(size_average=False)\n",
    "# average_size=False to match results in\n",
    "# manual approach, where we did not normalize\n",
    "# the cost by the batch size\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001 | Train ACC: 0.987 | Cost: 5.581\n",
      "Epoch: 002 | Train ACC: 0.987 | Cost: 4.882\n",
      "Epoch: 003 | Train ACC: 1.000 | Cost: 4.381\n",
      "Epoch: 004 | Train ACC: 1.000 | Cost: 3.998\n",
      "Epoch: 005 | Train ACC: 1.000 | Cost: 3.693\n",
      "Epoch: 006 | Train ACC: 1.000 | Cost: 3.443\n",
      "Epoch: 007 | Train ACC: 1.000 | Cost: 3.232\n",
      "Epoch: 008 | Train ACC: 1.000 | Cost: 3.052\n",
      "Epoch: 009 | Train ACC: 1.000 | Cost: 2.896\n",
      "Epoch: 010 | Train ACC: 1.000 | Cost: 2.758\n",
      "\n",
      "Model parameters:\n",
      "  Weights: \n",
      " 4.2267 -2.9613\n",
      "[torch.FloatTensor of size 1x2]\n",
      "\n",
      "  Bias: \n",
      "1.00000e-02 *\n",
      "  9.9419\n",
      "[torch.FloatTensor of size 1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def comp_accuracy(label_var, pred_probas):\n",
    "    pred_labels = custom_where((pred_probas > 0.5).float(), 1, 0).view(-1)\n",
    "    acc = torch.sum(pred_labels == label_var.view(-1)).float() / label_var.size(0)\n",
    "    return acc\n",
    "\n",
    "\n",
    "num_epochs = 10\n",
    "\n",
    "X_train_var = Variable(torch.Tensor(X_train), requires_grad=False)\n",
    "y_train_var = Variable(torch.Tensor(y_train), requires_grad=False).view(-1, 1)\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    #### Compute outputs ####\n",
    "    out = model(X_train_var)\n",
    "    \n",
    "    #### Compute gradients ####\n",
    "    cost = cost_fn(out, y_train_var)\n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    \n",
    "    #### Update weights ####  \n",
    "    optimizer.step()\n",
    "    \n",
    "    #### Logging ####      \n",
    "    pred_probas = model(X_train_var)\n",
    "    acc = comp_accuracy(y_train_var, pred_probas)\n",
    "    print('Epoch: %03d' % (epoch + 1), end=\"\")\n",
    "    print(' | Train ACC: %.3f' % acc, end=\"\")\n",
    "    print(' | Cost: %.3f' % cost_fn(pred_probas, y_train_var))\n",
    "\n",
    "\n",
    "    \n",
    "print('\\nModel parameters:')\n",
    "print('  Weights: %s' % model.linear.weight.data)\n",
    "print('  Bias: %s' % model.linear.bias.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set accuracy: 100.00%\n"
     ]
    }
   ],
   "source": [
    "X_test_var = Variable(torch.Tensor(X_test).type(dtype), requires_grad=False)\n",
    "y_test_var = Variable(torch.Tensor(y_test).type(dtype), requires_grad=False)\n",
    "\n",
    "pred_probas = model(X_test_var)\n",
    "test_acc = comp_accuracy(y_test_var, pred_probas)\n",
    "\n",
    "print('Test set accuracy: %.2f%%' % (test_acc*100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
